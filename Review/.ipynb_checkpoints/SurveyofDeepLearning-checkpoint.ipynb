{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Humans manually cateogrize and rely on expertise in order to classify MRI / PET / X-Ray scans this can pose an issue as Doctors can have an enormous amount of data to look over, and potential baises as well as fitigue could play a role in the classification process.\n",
    "\n",
    "In the past Doctors would use CAD systems, to assist them in this classification process. \"In the CAD systems, machine learning is able to extract informative features that describe the inherent patterns from data and play a vital role in medical image analysis\" (Zhang et al., 2020) \n",
    "\n",
    "**side note** I'm not entirely sure what this means, does it mean that we can use machine learning to extract features from CAD systems or does this mean that machine learning is built into those CAD systems? I would assume the former because in a previous paragraph they stated that these systems where around in the 1980s.\n",
    "\n",
    "Another problem we seem to run into is that the brain is highly complex and feature selection in real life is still done by doctors today, this results in a problem for someone who would want to use machine learning to solve one of these problems as we are not experts this would be difficult for us to do proper feature selection there are some algorithms that exist out there but it's still somewhat limited according to the paper.\n",
    "\n",
    "**side note** In data science this is usually the case that feature selection will be done by a human regardless of the domain this is why we need to understand more about the particular subject that we are doing research on.\n",
    "\n",
    "\"Compared to the traditional machine learning algorithms, deep learning automatically discovers the informative representations without the professional knowledge of domain experts\" (Zhang et al., 2020)\n",
    "\n",
    "**side note** So in our approach this would be really inefficent this is talking about unsupervised learning, in the case of unsupervised learning although it can discover features by itself without humans it takes a lot more time and a lot more data than a supervised learning method. From what I understand \"Deep Learning\" doesn't mean that it has to be unsupervised \"Deep Learning\" is just a classification of learning algorithm.\n",
    "\n",
    "It says that we can break down medical image analysis into several categories: classification, detection, registration, and segmentation. Classification aims to classify images into two or more categories \"the stacked auto-encoder model\" is an example of this classification. Detection consists of finding where the problem lies if there's a tumor for example the ability to identify where the tumor is. Segmentation is taking medical images and partitioning them into relevant parts i.e. tissue classes, organs etc. \"Registration of medical images is a process that searches for the correct alignment of images.\" (Zhang et al., 2020). *Not entirely sure what this means by \"alignment\".*\n",
    "\n",
    "Then there's also some other tasks that you can do such as content-based image retreival, image generation and image enhancement.\n",
    "\n",
    "The rest of the paper will be structured as follow **TODO add table of contents**:\n",
    "    - popular deep learning techniques for brain disorders\n",
    "    - detailed overview of recent studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "I will add some code here to enhance my understanding. Will be using torch as well as commented out Julia code.\n",
    "\n",
    "### MLPs and FFNN\n",
    "\n",
    "This is the most basic type of NN takes some hidden layers and passes a bunch of non linear transforms onto data each layer is fully connected to the next layer. Given some input $x$ the composition function $y_k$ could be written as \n",
    "\n",
    "$$\n",
    "y_k{(x;\\theta)} = f^{(2)}(\\Sigma^{m}_{j=1}w_{k,j}^{(2)}f^{(1)}(\\Sigma_{i=1}^{N}w_{j,i}^{(1)}x_i+b_j^{(1)})+b_{k}^{(2)})\n",
    "$$\n",
    "\n",
    "Where $f^{(n)}$ denotes a non linear activation function and $\\theta$ represents the parameters (Zhang et al., 2020) \n",
    "\n",
    "**side note** usually in a FFNN or MLP the non linear activation function is sigmoid $\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "A diagram of a Multi-layer Perceptron from the article:\n",
    "\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g001.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0157, -0.2693,  0.3979,  ...,  0.2230,  0.1475,  0.4930],\n",
      "        [ 0.0187, -0.2652,  0.4010,  ...,  0.2224,  0.1451,  0.4918],\n",
      "        [ 0.0182, -0.2685,  0.3991,  ...,  0.2231,  0.1461,  0.4958],\n",
      "        ...,\n",
      "        [ 0.0177, -0.2690,  0.3996,  ...,  0.2228,  0.1470,  0.4955],\n",
      "        [ 0.0177, -0.2709,  0.3986,  ...,  0.2208,  0.1451,  0.4943],\n",
      "        [ 0.0174, -0.2672,  0.3983,  ...,  0.2211,  0.1461,  0.4939]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# example of a basic MLP in torch using sequential\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.sigmoid_stack = nn.Sequential(\n",
    "            nn.Linear(128, 32), # input layer\n",
    "            nn.Sigmoid(),       # non linear transform\n",
    "            nn.Linear(32, 64),  # hidden layer 1\n",
    "            nn.Sigmoid(),       # non linear transform\n",
    "            nn.Linear(64, 64),  # hidden layer 2\n",
    "            nn.Sigmoid(),       # non linear transform\n",
    "            nn.Linear(64, 10)   # output layer\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # feed forward\n",
    "        logits = self.sigmoid_stack(x)\n",
    "        return logits\n",
    "\n",
    "x = torch.randn(128, 128) # fake data\n",
    "model = MLP()             # instatiate the model\n",
    "print(model.forward(x))   # feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "This is the optimization algorithm that makes neural networks useful although they go over this in the paper there's a really good video on this [here](https://www.youtube.com/watch?v=Ilg3gGewQ5U) by 3blue1brown. Basiclly you want to take the gradient of the loss function and move towards a minimum in N-d space. Where N is the number of features. i.e.\n",
    "\n",
    "$$\n",
    "\\nabla{\\ell(x_1, x_2, x_3, ..., x_i)} = [\\frac{\\partial\\ell}{\\partial x_1}, \\frac{\\partial\\ell}{\\partial x_2}, \\frac{\\partial\\ell}{\\partial x_3}, ... \\frac{\\partial\\ell}{\\partial x_i}]\n",
    "$$\n",
    "\n",
    "Example in pytorch find docs [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) for this example this is modified version not using data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()                                   # mean squared loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # does BP for us\n",
    "\n",
    "def train_model(model, loss_fn, optm, train_data):\n",
    "    for X, y in train_data:\n",
    "        # predict then compute loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # BP\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_model(model, loss_fn, test_data):\n",
    "    # size of data\n",
    "    size = len(test_data)\n",
    "    \n",
    "    # iteration\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "    \n",
    "    return (loss / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training and testing loop is very central to the entire process of data science I'd assume this is the same for Deep Learning as I mean otherwise, how are you going to optimize your models to be useful?\n",
    "\n",
    "**side note** logits is the unbounded value and is inverse of sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Auto Encoders\n",
    "\n",
    "*Encode decode similar to transformers?*\n",
    "\n",
    "Auto encoders by themselves are very limited due to the simple and shallow structure of an auto encoder iteself but when you stack these you can create a stacked auto encoder wow I know. This improves the model substantially.\n",
    "\n",
    "Lower layers learn more simple details (similar to earlier layers in a CNN) while higher layers will be able to extract more complex characteristics (similar to later layers in a CNN). There are many variations on Auto Encoders and they can also be stacked this has potential creating more useful and robust models.\n",
    "\n",
    "AE diagram from the article:\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g002.jpg)\n",
    "\n",
    "To avoid drawbacks of Gradient Decent \"the greedy layer-wise approach is considered to training parameters of an SAE\" (Zhang et al., 2020) this is probably most interesting part of this section as non gradient decent optimization methods have been proposed for a while now. **come back and look into this**\n",
    "\n",
    "code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1016, -0.0388,  0.4285, -0.0595,  0.1546])\n"
     ]
    }
   ],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        # single encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(5, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # single decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 5)\n",
    "        )\n",
    "    \n",
    "    # feed forward operation\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "# test case\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(5)\n",
    "    model = AE()\n",
    "    print(model.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Belief Networks (WIP)\n",
    "\n",
    "A deep belief network stacks multiple restricted boltzman machines (RBMs) for deep architecture construction. **First question**: what is a boltzman machine? A DBN has one visible layer and multipule hidden layers. Given a visible layer and N hidden layers we can construct the model. $f(v, hL_1, hL_2, ... hL_N)$ the eq. given is: \n",
    "\n",
    "$$\n",
    "P(v,h(1),…,h(L))=P(v|h(1))(∏_{l=1}^{L−2}P(h(l)|h(l+1)))P(h(L−1),h(L))\n",
    "$$\n",
    "\n",
    "so from what I read elsewhere as well as re-reading over this section a DBN is made up of the first two layers creating a RBM this is the $P(v|h(1))$ part of the eq. then the rest of the network is a directed network made up of the hidden layers we can see this in the diagram where the first two layers are connected via solid lines and the other layers are connected with arrows (although it's kind of hard to see at first) this is slightly different from an FFNN due to the RBM part of the DBN. *I beleive* \n",
    "\n",
    "A DBN **is not** a FFNN there are two parts to the training of a DBN the pre-training and fine-tuning.\n",
    "\"pretraining is done by stacking RBMs to find the parameter space.\" (Zhang et al., 2020) Take some n layer this layer is trained as a RBM via observation data from the n-1 layer.\n",
    "\n",
    "diagram from the paper:\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g003.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4283, 0.5327, 0.5573, 0.4256],\n",
      "        [0.4283, 0.5327, 0.5572, 0.4256],\n",
      "        [0.4283, 0.5327, 0.5573, 0.4256],\n",
      "        [0.4283, 0.5327, 0.5573, 0.4256]])\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, x: Tensor):\n",
    "        super(DBN, self).__init__()\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        self.input_layer = RBM()\n",
    "        \n",
    "        self.bernoulli = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def pre_train(self, x: Tensor) -> Tensor:\n",
    "        return self.input_layer.train(x)\n",
    "    \n",
    "    def fine_tuning(self) -> Tensor:\n",
    "        self.x = self.pre_train(x)\n",
    "        return self.bernoulli(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(4, 4)\n",
    "    model = DBN(x)\n",
    "    print(model.fine_tuning())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzman Machine\n",
    "\n",
    "This seems to be very important for understanding both DBMs and DBNs this is not gone over in the paper we get the simple $P(v|h(1))$ this is sort of useless if we don't know what $P$ is.\n",
    "\n",
    "On top of this RBMs are not some arbitrary thing like a normal distrobution or something this is a complex probability model and make up the layers of the DBM and the first 2 layers of the DBN.\n",
    "\n",
    "The joint probability distrobution of visible and hidden layers is repersented by $P(v, h) = \\frac{1}{Z}\\Sigma_{\\{h\\}}e^{-E(v, h)}$ where $E$ is the energy function between two variables $v$ and $h$ $E(v, h) = -a^{T}v-b^{T}h-v^{T}Wh$ and $Z$ is the partition function repersented as $Z=\\Sigma e^{-E(v,h)}$ conditional probability of $P(v|h)$ and conversely the conditional probability of $P(h|v)$ we can see that $P(v|h) = \\Pi P(v_i|h)$ and conversely for $P(h|v) = \\Pi P(h_i|v)$ this is also sort of useless so the individual conditional probability can be given by the form $P(h_i|v) = \\sigma (b_i + \\Sigma_j^n w_{j,i}v_j)$ and $P(v_i|h) = \\sigma (a_i + \\Sigma_j^n w_{i,j}h_j)$ putting all of this together we get that\n",
    "$$\n",
    "P(v|h(1)) = \\Pi_i^m \\sigma(a_i+\\Sigma_j^n w_{i,j}h_j)\n",
    "$$\n",
    "**Then we pass visible to a softmax?**\n",
    "This whole thing in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1427, 1.1427, 1.1427, 1.1427],\n",
      "        [1.3879, 1.3879, 1.3879, 1.3879],\n",
      "        [1.3763, 1.3763, 1.3763, 1.3763],\n",
      "        [1.0930, 1.0930, 1.0930, 1.0930]])\n"
     ]
    }
   ],
   "source": [
    "class RBM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RBM, self).__init__()\n",
    "        \n",
    "        self.visible = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def step(self, n: int, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.matmul(self.visible[n](x), self.hidden[n](x))\n",
    "    \n",
    "    def train(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        prod = torch.ones(4, 4)\n",
    "        for i in range(len(self.hidden)):\n",
    "            z = torch.matmul(self.visible[i](x), self.hidden[i](x))\n",
    "            prod = torch.matmul(z, prod)\n",
    "        return prod\n",
    "        \n",
    "\n",
    "with torch.no_grad():\n",
    "    model = RBM()\n",
    "    x = torch.randn(4, 4)\n",
    "    print(model.train(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Boltzman Machine\n",
    "\n",
    "The diagram from the DBNs B side is the Deep Boltzman Machine similar to the DBN except instead of putting one RBM we make the entire network out of RBMs stacked on top of each other. so it doesn't include a directed part like the DBN does the entire DBM is undirected. This also means no Bayseian Network.\n",
    "\n",
    "There's some interesting probability that comes with this instead of $P(v|hL_1)$ we now have $P(v|hL_{N-1}, hL_{N-2})$\n",
    "\n",
    "This sort of conflicts with the explination of pre-training, isn't the deep boltzman machine just a bunch of stacked RBMs like the pretraining method in a DBN? This is still a novel approach but I'm just kind of confused.\n",
    "\n",
    "Also this: \"Given the values of the neighboring layers, the conditional probabilities over the visible and the L set of hidden units are given by logistic sigmoid functions\" **theres eq.s here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0931, -0.0931, -0.0931, -0.0931],\n",
      "        [-0.0931, -0.0931, -0.0931, -0.0931],\n",
      "        [-0.0931, -0.0931, -0.0931, -0.0931],\n",
      "        [-0.0931, -0.0931, -0.0931, -0.0931]])\n"
     ]
    }
   ],
   "source": [
    "class DBM(nn.Module):\n",
    "    def __init__(self, n_layers=4):\n",
    "        super(DBM, self).__init__()\n",
    "        \n",
    "        self.layer = RBM()\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "# massive bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "The way the model works is based off of game theory where there's two players the discriminator and the generator, the generator wants to make better and better images and the discriminator wants to identify real from fake data. \n",
    "\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g004.jpg)\n",
    "\n",
    "$D(x)$ is the discriminator network which outputs the scalar probability that $x$ came from the training data rather than the generator. the output of $D(x)$ should be high when the $x$ comes from the training data and low when it comes from generated data we want this output to become $0.5$ so it cannot tell weather data came from the generator or the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-7f2b896975af>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-7f2b896975af>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    nn.ConvTranspose2d(12, 32, 4, bias=False),\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Generative Model\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.ConvTranspose2d(12, 32, 4, bias=False), # input layer (latent_space, features, kernel_size)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "            nn.ConvTranspose2d(12, 32, 4, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class D(nn.Module):\n",
    "    super(D, self).__init__()\n",
    "    \n",
    "    self.network = nn.Sequential(\n",
    "        nn.Conv2d(12, 32, 4, bias=False),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "One of the two major classes of neural networks that you hear about I already know about these and I assume this is what you would want to use for the particular task that we're doing CNNs are mostly used in Computer Vision tasks.\n",
    "\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g005.jpg)\n",
    "\n",
    "As we go deeper into the network the CNN learns more and more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "The other of the two major neural networks that you hear about this is mostly used for sequential data, ie text tasks like NLP are most common for RNNs. RNNs have some downsides so there's alternitive topologies that you usually see such as GRUs and LSTMs.\n",
    "\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g007.jpg)\n",
    "\n",
    "code implementation of an RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
