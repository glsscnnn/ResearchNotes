{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Humans manually cateogrize and rely on expertise in order to classify MRI / PET / X-Ray scans this can pose an issue as Doctors can have an enormous amount of data to look over, and potential baises as well as fitigue could play a role in the classification process.\n",
    "\n",
    "In the past Doctors would use CAD systems, to assist them in this classification process. \"In the CAD systems, machine learning is able to extract informative features that describe the inherent patterns from data and play a vital role in medical image analysis\" (Zhang et al., 2020) \n",
    "\n",
    "**side note** I'm not entirely sure what this means, does it mean that we can use machine learning to extract features from CAD systems or does this mean that machine learning is built into those CAD systems? I would assume the former because in a previous paragraph they stated that these systems where around in the 1980s.\n",
    "\n",
    "Another problem we seem to run into is that the brain is highly complex and feature selection in real life is still done by doctors today, this results in a problem for someone who would want to use machine learning to solve one of these problems as we are not experts this would be difficult for us to do proper feature selection there are some algorithms that exist out there but it's still somewhat limited according to the paper.\n",
    "\n",
    "**side note** In data science this is usually the case that feature selection will be done by a human regardless of the domain this is why we need to understand more about the particular subject that we are doing research on.\n",
    "\n",
    "\"Compared to the traditional machine learning algorithms, deep learning automatically discovers the informative representations without the professional knowledge of domain experts\" (Zhang et al., 2020)\n",
    "\n",
    "**side note** So in our approach this would be really inefficent this is talking about unsupervised learning, in the case of unsupervised learning although it can discover features by itself without humans it takes a lot more time and a lot more data than a supervised learning method. From what I understand \"Deep Learning\" doesn't mean that it has to be unsupervised \"Deep Learning\" is just a classification of learning algorithm.\n",
    "\n",
    "It says that we can break down medical image analysis into several categories: classification, detection, registration, and segmentation. Classification aims to classify images into two or more categories \"the stacked auto-encoder model\" is an example of this classification. Detection consists of finding where the problem lies if there's a tumor for example the ability to identify where the tumor is. Segmentation is taking medical images and partitioning them into relevant parts i.e. tissue classes, organs etc. \"Registration of medical images is a process that searches for the correct alignment of images.\" (Zhang et al., 2020). *Not entirely sure what this means by \"alignment\".*\n",
    "\n",
    "Then there's also some other tasks that you can do such as content-based image retreival, image generation and image enhancement.\n",
    "\n",
    "The rest of the paper will be structured as follow **TODO add table of contents**:\n",
    "    - popular deep learning techniques for brain disorders\n",
    "    - detailed overview of recent studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "I will add some code here to enhance my understanding. Will be using torch as well as commented out Julia code.\n",
    "\n",
    "### MLPs and FFNN\n",
    "\n",
    "This is the most basic type of NN takes some hidden layers and passes a bunch of non linear transforms onto data each layer is fully connected to the next layer. Given some input $x$ the composition function $y_k$ could be written as \n",
    "\n",
    "$$\n",
    "y_k{(x;\\theta)} = f^{(2)}(\\Sigma^{m}_{j=1}w_{k,j}^{(2)}f^{(1)}(\\Sigma_{i=1}^{N}w_{j,i}^{(1)}x_i+b_j^{(1)})+b_{k}^{(2)})\n",
    "$$\n",
    "\n",
    "Where $f^{(n)}$ denotes a non linear activation function and $\\theta$ represents the parameters (Zhang et al., 2020) \n",
    "\n",
    "**side note** usually in a FFNN or MLP the non linear activation function is sigmoid $\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "A diagram of a Multi-layer Perceptron from the article:\n",
    "\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g001.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0416, 0.3877, 0.0299,  ..., 0.2280, 0.1278, 0.2775],\n",
      "        [0.0414, 0.3876, 0.0308,  ..., 0.2254, 0.1290, 0.2735],\n",
      "        [0.0442, 0.3881, 0.0297,  ..., 0.2232, 0.1269, 0.2719],\n",
      "        ...,\n",
      "        [0.0470, 0.3909, 0.0282,  ..., 0.2222, 0.1286, 0.2685],\n",
      "        [0.0442, 0.3896, 0.0296,  ..., 0.2263, 0.1297, 0.2733],\n",
      "        [0.0429, 0.3901, 0.0300,  ..., 0.2244, 0.1290, 0.2711]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# example of a basic MLP in torch using sequential\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.sigmoid_stack = nn.Sequential(\n",
    "            nn.Linear(128, 32), # input layer\n",
    "            nn.Sigmoid(),       # non linear transform\n",
    "            nn.Linear(32, 64),  # hidden layer 1\n",
    "            nn.Sigmoid(),       # non linear transform\n",
    "            nn.Linear(64, 64),  # hidden layer 2\n",
    "            nn.Sigmoid(),       # non linear transform\n",
    "            nn.Linear(64, 10)   # output layer\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # feed forward\n",
    "        logits = self.sigmoid_stack(x)\n",
    "        return logits\n",
    "\n",
    "x = torch.randn(128, 128) # fake data\n",
    "model = MLP()             # instatiate the model\n",
    "print(model.forward(x))   # feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "This is the optimization algorithm that makes neural networks useful although they go over this in the paper there's a really good video on this [here](https://www.youtube.com/watch?v=Ilg3gGewQ5U) by 3blue1brown. Basiclly you want to take the gradient of the loss function and move towards a minimum in N-d space. Where N is the number of features. i.e.\n",
    "\n",
    "$$\n",
    "\\nabla{\\ell(x_1, x_2, x_3, ..., x_i)} = [\\frac{\\partial\\ell}{\\partial x_1}, \\frac{\\partial\\ell}{\\partial x_2}, \\frac{\\partial\\ell}{\\partial x_3}, ... \\frac{\\partial\\ell}{\\partial x_i}]\n",
    "$$\n",
    "\n",
    "Example in pytorch find docs [here](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) for this example this is modified version not using data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()                                   # mean squared loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # does BP for us\n",
    "\n",
    "def train_model(model, loss_fn, optm, train_data):\n",
    "    for X, y in train_data:\n",
    "        # predict then compute loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # BP\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_model(model, loss_fn, test_data):\n",
    "    # size of data\n",
    "    size = len(test_data)\n",
    "    \n",
    "    # iteration\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "    \n",
    "    return (loss / size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training and testing loop is very central to the entire process of data science I'd assume this is the same for Deep Learning as I mean otherwise, how are you going to optimize your models to be useful?\n",
    "\n",
    "**side note** logits is the unbounded value and is inverse of sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Auto Encoders\n",
    "\n",
    "*Encode decode similar to transformers?*\n",
    "\n",
    "Auto encoders by themselves are very limited due to the simple and shallow structure of an auto encoder iteself but when you stack these you can create a stacked auto encoder wow I know. This improves the model substantially.\n",
    "\n",
    "Lower layers learn more simple details (similar to earlier layers in a CNN) while higher layers will be able to extract more complex characteristics (similar to later layers in a CNN). There are many variations on Auto Encoders and they can also be stacked this has potential creating more useful and robust models.\n",
    "\n",
    "AE diagram from the article:\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g002.jpg)\n",
    "\n",
    "To avoid drawbacks of Gradient Decent \"the greedy layer-wise approach is considered to training parameters of an SAE\" (Zhang et al., 2020) this is probably most interesting part of this section as non gradient decent optimization methods have been proposed for a while now. **come back and look into this**\n",
    "\n",
    "code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1016, -0.0388,  0.4285, -0.0595,  0.1546])\n"
     ]
    }
   ],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "        # single encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(5, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # single decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 5)\n",
    "        )\n",
    "    \n",
    "    # feed forward operation\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "# test case\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(5)\n",
    "    model = AE()\n",
    "    print(model.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Belief Networks (WIP)\n",
    "\n",
    "A deep belief network stacks multiple restricted boltzman machines (RBMs) for deep architecture construction. **First question**: what is a boltzman machine? A DBN has one visible layer and multipule hidden layers. Given a visible layer and N hidden layers we can construct the model. $f(v, hL_1, hL_2, ... hL_N)$ the eq. given is: \n",
    "\n",
    "$$\n",
    "P(v,h(1),…,h(L))=P(v|h(1))(∏_{l=1}^{L−2}P(h(l)|h(l+1)))P(h(L−1),h(L))\n",
    "$$\n",
    "\n",
    "so from what I read elsewhere as well as re-reading over this section a DBN is made up of the first two layers creating a RBM this is the $P(v|h(1))$ part of the eq. then the rest of the network is a directed network made up of the hidden layers we can see this in the diagram where the first two layers are connected via solid lines and the other layers are connected with arrows (although it's kind of hard to see at first) this is slightly different from an FFNN due to the RBM part of the DBN. *I beleive*\n",
    "\n",
    "diagram from the paper:\n",
    "![](https://www.frontiersin.org/files/Articles/560709/fnins-14-00779-HTML/image_m/fnins-14-00779-g003.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DBN, self).__init__()\n",
    "        \n",
    "        # RBM\n",
    "        self.input_layer\n",
    "        \n",
    "        self.bernoulli = nn.Sequential(\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(4, 4)\n",
    "        )\n",
    "        \n",
    "    def RBM(self, x):\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        x = self.RBM(x)\n",
    "        return self.bernoulli(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Boltzman Machine\n",
    "\n",
    "The diagram from the DBNs B side is the Deep Boltzman Machine similar to the DBN except instead of putting one RBM we make the entire network out of RBMs stacked on top of each other. so it doesn't include a directed part like the DBN does the entire DBM is undirected.\n",
    "\n",
    "There's some interesting probability that comes with this instead of $P(v|hL_1)$ we now have $P(v|hL_{N-1}, hL_{N-2})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
